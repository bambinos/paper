{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/ESCS.csv\")\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rho_{X_j Y\\cdot \\mathbf{X}_{-j}} = \n",
    "\\beta_j \\frac{\\text{sd}(X_j)}{\\text{sd}(Y)}\n",
    "\\sqrt{\\frac{1-R_{X_j \\mathbf{X}_{-j}}^2}{1-R_{Y \\mathbf{X}_{-j}}^2}}\n",
    "$$\n",
    "\n",
    "and the HDI for $\\beta_o$ in the manuscript goes from - 0.053 to 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_y = np.std(data.drugs)\n",
    "sd_x = np.std(data.o)\n",
    "r2_num = sm.OLS(endog = data.o, exog = sm.add_constant(data[['c', 'e', 'a', 'n']])).fit().rsquared\n",
    "r2_denom = sm.OLS(endog = data.drugs, exog = sm.add_constant(data[['c', 'e', 'a', 'n']])).fit().rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower partial correlation: -1.7168055086158238\n",
      "Upper partial correlation: 1.619627838316815\n"
     ]
    }
   ],
   "source": [
    "constant = (sd_x / sd_y) *  np.sqrt((1 - r2_num) / (1 - r2_denom))\n",
    "print('Lower partial correlation: ' + str(-0.053 * constant))\n",
    "print('Upper partial correlation: ' + str(0.05 * constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this weird result I wrote an email to Jake Westfall (the one who came up with the idea for priors in Bambi). And he respondend the following \n",
    "\n",
    "---\n",
    "\n",
    "Hi Tomas,\n",
    "\n",
    "I think this is actually a manifestation of an issue that you mentioned as a comment in the Overleaf doc (at least I think it was you, comment is gone now): it's not quite right to treat sd(Y) and R^2_YX as constants. In fact since these are both functions of Y, which is itself a function of β_j, then these both are functions of β_j as well. If you ignore that by treating sd(Y) and R^2_YX as fixed then you can get impossible values for ρ_j.\n",
    "\n",
    "If I'm correct that this is the root cause, then the same issue can be seen even more clearly in the case of a simple regression model, no covariates, where we want to convert between the slope, β, and the correlation between Y and X, ρ. Let the regression model be\n",
    "\n",
    "Y = βX + e\n",
    "\n",
    "and we assume X and e are uncorrelated. Now we can write the correlation as\n",
    "\n",
    "ρ = β sqrt[var(X) / var(Y)].\n",
    "\n",
    "For a numerical example, suppose β = 2 and var(X) = 1. Then var(Y) = 5, since\n",
    "\n",
    "var(Y) = β² var(X) + var(e)\n",
    "\n",
    "from taking the variance of equation 1. So if we plug these three values into the ρ equation above we see that the correlation is 2/sqrt(5) = 0.894. So far so good.\n",
    "\n",
    "But suppose we want to see what the correlation would be if the slope were larger, say β = 3. Simply setting β = 3 in equation 2 leads to ρ = 1.342, which is impossible. The problem is that increasing β also increases var(Y), as shown in the var(Y) equation above, so this needs to be taken into account as well. To do so, plug equation 3 into equation 2 and then set both β = 3 in both places where β now appears in the expanded equation. Then you get the correct answer of ρ = 0.949.\n",
    "\n",
    "So back to our actual case, I think the way forward is to take our equation for ρ_j and rewrite both sd(Y) and R^2_YX as functions of β_j, then plug the HDI endpoint values into this expanded equation. I don't have those identities on hand, but one could work through the algebra to obtain them...\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:** I don't see where we have $R^2_{YX}$ in our formula for the partial correlation. We have $R_{X_j \\mathbf{X}_{-j}}^2$ and $R_{Y \\mathbf{X}_{-j}}^2$ and I don't see, at least easily, where we have $\\beta_j$ in there. Maybe it can appear if we start to dig into correlations involved in the $R^2$ values? The following is how I tried to update the computation, and the results make more sense now. However I'm still not 100% satisfied.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with... \n",
    "\n",
    "$$\n",
    "\\rho_{X_j Y\\cdot \\mathbf{X}_{-j}} = \n",
    "\\beta_j \\sqrt{\\frac{\\text{var}(X_j)}{\\text{var}(Y)}}\n",
    "\\sqrt{\\frac{1-R_{X_j \\mathbf{X}_{-j}}^2}{1-R_{Y \\mathbf{X}_{-j}}^2}}\n",
    "$$\n",
    "\n",
    "where we set \n",
    "\n",
    "$$\n",
    "\\text{var}(Y) = \\beta_1^2\\text{var}(X_1) + \\cdots + \\beta_j^2\\text{var}(X_j) + \\cdots \\beta_p^2\\text{var}(X_p) + \\sigma_\\epsilon^2\n",
    "$$\n",
    "\n",
    "so we include the dependency of $\\text{var}(Y)$ on $\\beta_j$. However, some caution points...\n",
    "\n",
    "* We still consider coefficient of determinations to be constant\n",
    "* We still consider betas to be constant (when they are random in the Bayesian model)\n",
    "\n",
    "We know we see $\\beta_j$ to -0.053 and 0.05, but what do we do with the other values? Let's use the mean values from their priors (we're somehow assuming independence, as we did when deriving var(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/drugs_prior.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to set all the betas to 0 (prior mean) except from $\\beta_o$ which takes -0.053 and 0.05. For $\\sigma_\\epsilon$ we have 0.65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8604584186710289\n",
      "0.8473344951564157\n"
     ]
    }
   ],
   "source": [
    "sq_sigma_epsilon = 0.65 ** 2\n",
    "var_y_lower = (-0.053 * sd_x) ** 2 + sq_sigma_epsilon\n",
    "var_y_upper = (0.05 * sd_x) **2 + sq_sigma_epsilon\n",
    "\n",
    "constant_lower = (sd_x / var_y_lower ** 0.5) *  np.sqrt((1 - r2_num) / (1 - r2_denom))\n",
    "constant_upper = (sd_x / var_y_upper ** 0.5) *  np.sqrt((1 - r2_num) / (1 - r2_denom))\n",
    "print(-0.053 * constant_lower)\n",
    "print(0.05 * constant_upper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmb",
   "language": "python",
   "name": "bmb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
